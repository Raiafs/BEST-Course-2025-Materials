{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce02a8e8",
   "metadata": {},
   "source": [
    "## Neural Network training and evaluation\n",
    "\n",
    "In this notebook, you will explore and train a Neural Network classifier, using the dataset provided and the features extracted previously.\n",
    "The Neural Network we'll be using is the one from PyTorch.\n",
    "\n",
    "To get started, we have to import some tools to train, test, save and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe75923",
   "metadata": {},
   "source": [
    "### Loading the training data and creating a train testing split\n",
    "\n",
    "Using the pickle library, we can retrieve the features we extracted in the feature extraction stage.\n",
    "The format of the data that is stored is (X,y) where:\n",
    "- X is the list of feature vectors.\n",
    "- y is the list of labels.\n",
    "- The label in position y[i] is respective to the feature vector in X[i].\n",
    "\n",
    "After loading, we create a train_test split. The split is done randomly (we set a random_state to make it deterministic so you can run this cell multiple times), with 20% being set for testing and the remaining for training.\n",
    "We will use the training for the model training and the testing to evaluate the trained models.\n",
    "\n",
    "Because PyTorch uses algorithms specialized to run the calculations necessary, we have to provide data in a specific structure, DataLoader.\n",
    "- train_loader is the one we use for training.\n",
    "- val_loader is the one we use for evaluation and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c24995",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_vectors = \"training_vectors.pkl\"  # Path for the .pkl containing the extracted training data\n",
    "batch_size = 32 #You can go with 32 or 16, that's what we recommend\n",
    "\n",
    "with open(training_vectors, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Assuming the pickle contains a tuple: (X, y)\n",
    "X, y = data\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890ccc4",
   "metadata": {},
   "source": [
    "### Building the Neural Network\n",
    "\n",
    "Neural Networks can have wildly different architectures, so this will be your playground to improve the model.\n",
    "The architecture itself is defined inside the \\_\\_init\\_\\_, where you can determine the number of layers, their size (hidden_dim), activation functions.\n",
    "We then have a forward function. This function is what it's used for the feed forward of the neural network, that will be used to classify the vector.\n",
    "\n",
    "The example we have below has the following architecture:\n",
    "- An input layer, receiving the features vector.\n",
    "- 2 hidden layers with hidden_dim size (defined by you).\n",
    "- An output layer, that will result in a vector of the classes that will determine the class of that vector.\n",
    "- All transitions between layers use the Tanh (hyperbolic tangent) activation function.\n",
    "\n",
    "You can shape this model as much as you want! Some ideas are:\n",
    "- The number and size of of layers can be a factor.\n",
    "- The activation functions: ReLU(), LeakyReLU(), GELU(), Sigmoid()\n",
    "- You can add a dropout in the NN (at the end, for example).\n",
    "\n",
    "Keep in mind Neural networks take time to train, so be sure to not exagerate the number of architectures.\n",
    "Make sure to explore, search the internet and ask around to understand which changes would be interesting. Chatbots are good at giving ideas and you can look into them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the input size and num_classes according to:\n",
    "\n",
    "#architecture of the Neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),           # Go from input to hidden layer 1\n",
    "            nn.Tanh(),                                  # Activation Function\n",
    "            nn.Linear(hidden_dim, hidden_dim),          # Go from hidden layer 1 to 2\n",
    "            nn.Tanh(),                                  # Activation Function\n",
    "            nn.Linear(hidden_dim, num_classes),         # Go from hidden layer 2 to output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba4e4a",
   "metadata": {},
   "source": [
    "### Decision Tree Training\n",
    "\n",
    "Now we create a decision tree (dt) with set parameters, and then train it using the fit() method.\n",
    "By playing with the parameters, you can improve the quality of your model, so play with the parameters.\n",
    "For each model you train, make sure to change the output_model_path, so you save all of them in different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60850663",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1      #Size of the feature vector\n",
    "num_classes = 2     #Classes that will be in the output\n",
    "hidden_dim = 200    #Hidden layers size (you can make this vary by specifying it in the architecture)\n",
    "num_epochs = 1000   #Epochs of training the NN\n",
    "\n",
    "model = SimpleNN(input_size, hidden_dim, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Raw logits expected\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} - \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} - \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"simple_nn_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034d269",
   "metadata": {},
   "source": [
    "### Neural Network Evaluation\n",
    "\n",
    "Now that we have trained some models, you can evaluate them and compare their metrics.\n",
    "We have accuracy as an example, however we encourage to explore multiple metrics, such as accuracy, precision, recall, f1-score, evaluate the confusion matrix or calculate the AUC.\n",
    "\n",
    "To do it, we load the model and test it by:\n",
    "- Using the model to predict the labels of the X_test vector.\n",
    "- Compare the actual labels (all_targets) with the predicted labels (all_preds) through the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cf07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_size, hidden_dim, num_classes)\n",
    "model.load_state_dict(torch.load(\"simple_nn_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Optional: move to GPU if used in training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Run predictions on the validation set\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(labels.numpy())\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(all_targets, all_preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
